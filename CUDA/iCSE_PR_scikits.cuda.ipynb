{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fft/ifft..\n",
      "Success status:  True\n",
      "Testing in-place fft..\n",
      "Success status:  True\n"
     ]
    }
   ],
   "source": [
    "# %load https://raw.github.com/lebedov/scikits.cuda/master/demos/fft_demo.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates how to use the PyCUDA interface to CUFFT to compute 1D FFTs.\n",
    "\"\"\"\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import numpy as np\n",
    "\n",
    "import skcuda.fft as cu_fft\n",
    "\n",
    "print 'Testing fft/ifft..'\n",
    "N = 4096*16\n",
    "\n",
    "x = np.asarray(np.random.rand(N), np.float32)\n",
    "xf = np.fft.fft(x)\n",
    "y = np.real(np.fft.ifft(xf))\n",
    "\n",
    "x_gpu = gpuarray.to_gpu(x)\n",
    "xf_gpu = gpuarray.empty(N/2+1, np.complex64)\n",
    "plan_forward = cu_fft.Plan(x_gpu.shape, np.float32, np.complex64)\n",
    "cu_fft.fft(x_gpu, xf_gpu, plan_forward)\n",
    "\n",
    "y_gpu = gpuarray.empty_like(x_gpu)\n",
    "plan_inverse = cu_fft.Plan(x_gpu.shape, np.complex64, np.float32)\n",
    "cu_fft.ifft(xf_gpu, y_gpu, plan_inverse, True)\n",
    "\n",
    "print 'Success status: ', np.allclose(y, y_gpu.get(), atol=1e-6)\n",
    "\n",
    "print 'Testing in-place fft..'\n",
    "x = np.asarray(np.random.rand(N)+1j*np.random.rand(N), np.complex64)\n",
    "x_gpu = gpuarray.to_gpu(x)\n",
    "\n",
    "plan = cu_fft.Plan(x_gpu.shape, np.complex64, np.complex64)\n",
    "cu_fft.fft(x_gpu, x_gpu, plan)\n",
    "\n",
    "cu_fft.ifft(x_gpu, x_gpu, plan, True)\n",
    "\n",
    "print 'Success status: ', np.allclose(x, x_gpu.get(), atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 16 ms, total: 24 ms\n",
      "Wall time: 24.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "cu_fft.fft(x_gpu, x_gpu, plan)\n",
    "x_gpu.get()[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 580 ms, sys: 24 ms, total: 604 ms\n",
      "Wall time: 602 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "xfft = np.fft.fft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153.03551-194.81418j)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpu.get()[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing matrix multiplication for type float32\n",
      "Success status:  True\n",
      "Testing vector multiplication for type float32\n",
      "Success status:  True\n",
      "Testing matrix multiplication for type complex64\n",
      "Success status:  True\n",
      "Testing vector multiplication for type complex64\n",
      "Success status:  True\n",
      "Testing matrix multiplication for type float64\n",
      "Success status:  True\n",
      "Testing vector multiplication for type float64\n",
      "Success status:  True\n",
      "Testing matrix multiplication for type complex128\n",
      "Success status:  True\n",
      "Testing vector multiplication for type complex128\n",
      "Success status:  True\n"
     ]
    }
   ],
   "source": [
    "# %load https://github.com/lebedov/scikits.cuda/raw/master/demos/dot_demo.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates multiplication of two matrices on the GPU.\n",
    "\"\"\"\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "\n",
    "import skcuda.linalg as culinalg\n",
    "import skcuda.misc as cumisc\n",
    "culinalg.init()\n",
    "\n",
    "# Double precision is only supported by devices with compute\n",
    "# capability >= 1.3:\n",
    "import string\n",
    "demo_types = [np.float32, np.complex64]\n",
    "if cumisc.get_compute_capability(pycuda.autoinit.device) >= 1.3:\n",
    "    demo_types.extend([np.float64, np.complex128])\n",
    "\n",
    "for t in demo_types:\n",
    "    print 'Testing matrix multiplication for type ' + str(np.dtype(t))\n",
    "    if np.iscomplexobj(t()):\n",
    "        a = np.asarray(np.random.rand(10, 5)+1j*np.random.rand(10, 5), t)\n",
    "        b = np.asarray(np.random.rand(5, 5)+1j*np.random.rand(5, 5), t)\n",
    "        c = np.asarray(np.random.rand(5, 5)+1j*np.random.rand(5, 5), t)\n",
    "    else:\n",
    "        a = np.asarray(np.random.rand(10, 5), t)\n",
    "        b = np.asarray(np.random.rand(5, 5), t)\n",
    "        c = np.asarray(np.random.rand(5, 5), t)\n",
    "\n",
    "    a_gpu = gpuarray.to_gpu(a)\n",
    "    b_gpu = gpuarray.to_gpu(b)\n",
    "    c_gpu = gpuarray.to_gpu(c)\n",
    "\n",
    "    temp_gpu = culinalg.dot(a_gpu, b_gpu)\n",
    "    d_gpu = culinalg.dot(temp_gpu, c_gpu)\n",
    "    temp_gpu.gpudata.free()\n",
    "    del(temp_gpu)\n",
    "    print 'Success status: ', np.allclose(np.dot(np.dot(a, b), c) , d_gpu.get())\n",
    "\n",
    "    print 'Testing vector multiplication for type '  + str(np.dtype(t))\n",
    "    if np.iscomplexobj(t()):\n",
    "        d = np.asarray(np.random.rand(5)+1j*np.random.rand(5), t)\n",
    "        e = np.asarray(np.random.rand(5)+1j*np.random.rand(5), t)\n",
    "    else:\n",
    "        d = np.asarray(np.random.rand(5), t)\n",
    "        e = np.asarray(np.random.rand(5), t)\n",
    "\n",
    "    d_gpu = gpuarray.to_gpu(d)\n",
    "    e_gpu = gpuarray.to_gpu(e)\n",
    "\n",
    "    temp = culinalg.dot(d_gpu, e_gpu)\n",
    "    print 'Success status: ', np.allclose(np.dot(d, e), temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU2, Python 2",
   "language": "python2",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
