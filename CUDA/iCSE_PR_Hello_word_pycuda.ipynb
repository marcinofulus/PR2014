{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programowanie masywnie równoleglych procesorów\n",
    "\n",
    "Wstęp\n",
    "=====\n",
    "\n",
    "\n",
    "Trendy w rozwoju CPU GPU \n",
    "--------------------\n",
    "\n",
    "\n",
    "W XX wieku procesory graficzne służyły do transformacji grafiki, głównie dwuwymiarowej. Potrzeby przemysłu związanego z nowoczesnymi grami wykorzystującymi grafikę trójwymiarową doprowadziły do intensywnego rozwoju i co równie ważne - upowszechnienia się,  wyspecjalizowanych procesorów  zwanych GPU (Graphics Processor Units), zdolnych renderować coraz bardziej skomplikowane sceny. W 2001 wraz z powstaniem technologii Pixel Shader i Vertex Shader zaczęto wykorzystywać procesor graficzny do obliczeń niezwiązanych z grafiką, określane mianem GPGPU -  General-Purpose computing on Graphics Processor Units. \n",
    "\n",
    "W 2004, firma Ageia zaprezentowała przełomowy produkt, znany pod nazwą PhysX a będący sprzętowym akceleratorem obliczeń związanych z symulacjami fizyki w grach komputerowych. Po co fizyka w grach? Rozwój grafiki 3D szybko doprowadził do sytuacji w której komputer mógł całkiem rozsądnie wyrenderować żądaną scenę. Jednak aby to zrobił musi wiedzieć co renderować. Po części informacje o położeniu objektów w grach są zapisywane z odgrywanych przez aktorów scen lub wprowadzane ręcznie przez animatora. Jednak ma to wiele wad: jest kosztowne, zabiera duzo pamięci a powtarzalność ruchów negatywnie wpływa na odbiór akcji. Dlatego optymalnym rozwiązaniem było generowanie scen przez pewne algorytmy. I tu przeszkodą stał się nasz mózg, który jest siecią neuronową  doskonale znającą podstawowe prawa fizyki! Bez trudu potrafimy odgadnąć jak ruszał by się słoń, gdyby był ze styropianu, znamy prawa odbicia, wiemy jak przepływa ciecz itp. Nie łatwo nas oszukać - więć programiści gier komputerowych postanowili nie \"generować\", ale (prawie uczciwie) symulować świat w grach. Jednak, aby obliczyć trajektorie na przykład miliona liści opadających z drzewa, potrzeba ogromnych mocy obliczeniowych. Z drugiej strony gra wymaga wykonania tych obliczeń w pewnym ogranicznonym czasie. Właśnie ta potrzeba, wraz z siedemdziesięcio-miliardowym biznesem gier, doprowadziła opracowania koncepcji sprzętowego akceleratora fizyki- tzw. PPU (Physics Processing Unit).\n",
    "\n",
    "Co się stało, że firmy Ageia nie ma już na runku? Okazało się, że producenci kart graficznych z procesorami GPU, zauważyli, że jest to ich branża. Ponadto, okazało się, że architektura GPU jest we wielu aspektach zliżona do architektury PPU. GPU wykonuje równoległe obliczenia na \"pixelach\", PPU wykonuje pewne obliczenia na wielu cząstkach. Firma Nvidia wykonała w 2006 kilka kroków - po pierwsze udostępniła standart programowania jednostek GPU zwany CUDA i zachęciła świat nauki to rozwijania oprogramowania. Po drugie wykupiła Agei-ę i zaimplementowała bibliotekę PhysX w CUDA, wobec czego jej akceleracja była możliwa nie tylko z procesorami PPU, ale na każdej nowej karcie firmy Nvidia. Praktycznie od momentu pojawienia się generacji GeForce serii 8, wszystkie procesory GPU z Nvidii są kompatybilne z CUDA i mogą wykonywać równolegle napisane programy. Różnice pomiędzy procesorami polegają głównie na ilości rdzeni, zwanych \"CUDA -cores\", których liczba waha się od 16 do 2500!  \n",
    "\n",
    "Moc obliczeniowa procesorów GPU a także, co jest niesłychanie ważne - przepustowość pamięci są o wiele większe od parametrów dostępnych na CPU. Oczywiście jest to kosztem ograniczenia możliwości procesorów GPU. Aby zdać sobie sprawę z liczb, warto przyjrzeć się grafikom opublikowanym przez Nvidię:\n",
    "\n",
    "![Teoretyczna wydajność obliczeniowa](files/figs/floating-point-operations-per-second.png \"Teoretyczna wydajność obliczeniowa\") \n",
    "\n",
    "![Przepustowość pamięci RAM ](files/figs/memory-bandwidth.png \"Przepustowość pamięci RAM \") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektura GPU \n",
    "\n",
    "\n",
    "Jak już się dowiedzieliśmy, nowoczesne procesory graficzne to urządzenia obliczeniowe, zdolne do wykonywania trylionów operacji zmiennoprzecinkowych  na sekundę. \n",
    " \n",
    "Na dzień dzisiejszy dostępne urządzenia GPU działające w standardzie CUDA można podzielić na trzy generacje o nazwach kodowych: Tesla, Fermi i Kepler. Każda z nich oferuje coraz bardziej zaawansowane funkcje i lepszą wydajność.\n",
    "\n",
    "Procesor GPU jest zorganizowany wokół koncepcji multiprocesorów strumieniowych (MP).\n",
    "Takie multiprocesorowy składają się z kilku-kilkunastu procesorów skalarnych (SP), z których każdy jest zdolny do wykonywania wątku w SIMT (Single Instruction, Multiple Threads).  Każdy MP posiada również ograniczoną ilość wyspecjalizowanej pamięci on-chip: zestaw rejestrów 32-bitowych, wspólny blok pamięci i pamięci podręcznej L1, cache stałych i cache tekstur. Rejestry są logicznie lokalne dla procesora skalarnego, ale inne typy pamięci są wspólne dla wszystkich SPs w każdym w MP. Umożliwia to m.in.  wymianę danych pomiędzy wątkami operującymi na tym samym MP.\n",
    "\n",
    "\n",
    "![Schemat procesora GPU](files/figs/GF100_03.jpg \"Schemat procesora GPU\") \n",
    "\n",
    "## CUDA - jako warstwa abstakcji w dostępie do GPU\n",
    "\n",
    "Procesory graficzne mogą różnić się ilością i organizacją rdzeni obliczeniowych (CUDA-cores). Implementacja algorytmów w  równoległy sposób powinienna być niezależna od konkrentego sprzętu na którym się one wykonują. W ogólności jest to zagadnienie - jak się wydaje - nierozwiązywalne, ale technologia CUDA uczyniła olbrzymi krok w kierunku uzyskania niezależności od sprzętu.  \n",
    "\n",
    "Po pierwsze programując na GPU tworzymy dużo więcej wątków niż jest fizycznie dostępnych rdzeni obliczeniowych. System kolejkowania wykonań zajmuje się częściową serializacją wywołań, tym intensywniejszą im gorszy sprzęt posiadamy. \n",
    "\n",
    "Po drugie dostęp do sprzętu jest realizowany przez  warstwe abstrakcji, zwaną właśnie CUDA, która zawiera w sobie informacje o hierarchii pamięci. Programista, nie przejmuje się tym ile rdzeni wykonuje jego program, ale wie, że pewne grupy wątków mają dostęp do wspólnej szybkiej pamięci. \n",
    "\n",
    "Programy napisane na CUDA dramatycznie różnią się od wielowątkowych równoległych odpowiedników napisanych na nawet kilkunastordzeniowe jednostki CPU. Z jednej strony jest to ograniczenie, skutkujące koniecznością przepisania wszystkich algorytmów. Jednak poztywnym efektem programowania na takiej architekturze jest ogromna kompatybilność i przenośność kodu na współczesne i z dużym prawdopodobieństwem przyszłe urządzenia.\n",
    "\n",
    "\n",
    "### Hierarchia pamieci\n",
    "\n",
    "\n",
    "Być może najbardziej istotną cechą architektury CUDA jest hierarchia pamięci\n",
    "z różnicą czasów dostępu o 1-2 rzędy wielkości pomiędzy kolejnymi poziomami. Najwolniejszą z punktu widzenia GPU jest pamięć RAM komputera gospodarza (host). Pamięć ta jest  oddzielona od procesora graficznego magistralą PCIe z teoretyczną maksymalną przepustowością w jednym kierunku 16 GB/s (PCI Express 3.0,x16).\n",
    "\n",
    "Następną w kolejności  jest globalna pamięć urządzenia GPU, który jest obecnie ograniczony do kilku gigabajtów (najnowsze karty mają juz 12GB) i o szerokości pasma około 100-200 ~ Gb/s. Jest to bardzo duża wartość, jednak dostęp do globalnej  pamięci jest operacją wysokiej latencji wynoszącą  kilkaset cykli zegara GPU.\n",
    "\n",
    "Najszybszym rodzajem pamięci dostępnym obecnie na GPU jest pamięć współdzielona (shared memory) znajdująca się bezpośrednio na multiprocesorze. Jest ona obecnie ograniczona  do 48 kB, ale ma przepustowość około ~1,3 TB/s. Co ciekawsze, latenacja w dostępie do tej pamięci jest bardzo mała - podobna jak dostęp do rejestrów jednostek SP.\n",
    "\n",
    "Powyższy opis łatwo sugeruje strategię pisania wydajnych programów na CUDA, a które można streścić jako: przenieść jak najwięcej danych, jak to możliwe\n",
    "do najszybszych rodzaju dostępnej pamięci i przechowywać je tam tak długo, jak to możliwe, przy jednoczesnej minimalizacji dostępu do wolniejszych rodzajów pamięci. Dodatkowo, jeśli dostęp do wolniejszej pamięci jest konieczny, można próbować wykorzystać \"wolny\" czas na wykonanie operacji arytmetycznych na pozostałych wątkach.\n",
    "\n",
    "\n",
    "\n",
    "### Wątki\n",
    "\n",
    "\n",
    "W programowaniu na CUDA czy OpenCL wykorzystujemy wątki. Co jest niespotykane w pisaniu kodu HPC na zwykłe procesory, wątków powinno się włączać o wiele więcej od dostępnych fizycznych rdzeni procesora. W praktyce programowania na CPU znany jest fakt, że nadmierne rozmnożenie wątków zazwyczaj spowalnia wykonanie programu, ze względu na \"context switching\", który może prowadzić do nieefektywnego wykorzystania pamieci cache jednostki centralnej a ponadto prowadzi w nieunikniony sposób do zmniejszenia dostępnej pamięci RAM.  W programowaniu w standardzie CUDA wątki są lekkie, ich przełączanie nie zabiera więcej niż pojedyńczych cykli procesora. Co więcej, nadmiarowe wątki potrafią mieć pozytywny efekt na wydajność, gdyż mogą przesłonić latencję dostępu do pamięci głównej.\n",
    "\n",
    "Z punktu widzenia użytkownika, programy CUDA są zorganizowane w jądrach. Jądro\n",
    "Jest to funkcja, która jest wykonywana wielokrotnie, jednocześnie na różnych multiprocesorach. Każda instancja jądra zwana jest wątkiem i jest przydzielana do pojedyńczego procesora skalarnego (SP). Wątki są  grupowane w jedno-, dwu - lub trójwymiarowe bloki przypisanych do multiprocesorów jeden do jednego - czyli mamy gwarancję, że w ramach jednego bloku jesteśmy na tym samym MP.\n",
    "\n",
    "###  Język programowania i struktura kursu\n",
    "\n",
    "*Kernele* pisze się w okrojonym dialekcie C/C++, zwanym CUDA-C. Referencyjnym i stosunkowo przystępnie napisanym dokumentem jest CUDA C Programming Guide, który jest dostępny pod adresem: http://docs.nvidia.com/cuda/cuda-c-programming-guide/\n",
    "W tym kursie nie będziemy systematycznie analizować wszystkich elementów CUDA-C. Postąpimy inaczej. Pokażemy na przykładach typowe zastosowania programowania GPU, które są niezwykle przydatne w fizyce. Rozwiązanie własnego problemu, proponujemy rozpocząć od znalezienia stosownego przykładu i próbie samodzielnego dostosowania jego kodu. Jest to rozwiązanie typu \"crash course\", które nie zastąpi systematycznego kursu. Jednak w dużej większości  przypadków, doświadczenie nabyte podczas zabawy z zamieszczonymi przykładami umożliwi optymalne rozwiązanie napotkanego problemu.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sposób pracy\n",
    "\n",
    "## Interaktywność - używamy interfejsu pyCUDA\n",
    "\n",
    "\n",
    "W tych materiałach zostanie zaprezentowane podejście, które pozwoli maksymalnie uprościć drogę do efektywnej pracy w CUDA. W tym celu zostanie zastosowany pakiet pyCUDA, który umożliwi pracę interaktywną z urządzeniem CUDA bez kompromisu jeśli chodzi o wydajność. W pyCUDA, sposób pracy sprowadza się do napisania jądra obliczeniowego w języku C z rozszerzeniami CUDA. Wywołanie jądra, jego kompilacja oraz inspekcja danych, w tym transfer do i z urzadzenia, wykonuje się w wygodnym języku python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierwszy program pyCUDA\n",
    "\n",
    "\n",
    "Przedstawimy teraz pierwszy program, który będzie napisany w CUDA, z użyciem pyCUDA. Przykład ten będzie bardzo prosty: mnożenie wektora przez liczbę. Na tym prostym przykładzie poznamy sposób pracy, który potem będziemy mogli użyć jako szablonu do tworzenia bardziej zaawansowanych programów. \n",
    "\n",
    "### Inicjalizacja\n",
    "\n",
    "Najprostszy sposób inicjalizacji urządzenia GPU tak by móc go dalej używać wygląda następująco:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz modułow z pakietu pyCUDA, importujemy również numpy. Jest to niezwykle istotne, bowiem tablice numpy to podstawowa struktura danych którą będziemy przesyłać na urządzenie GPU i z powrotem.\n",
    "\n",
    "Utwórzmy wektor danych, na przykład wykorzystując `linspace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
       "        12.,  13.,  14.,  15.,  16.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = np.linspace(1,16,16).astype(np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem naszego programu będzie pomnożenie wszystkich elementów tej tablicy przez zadaną liczbę. \n",
    "\n",
    "Tablica `a` znajduje się w pamięci systemu gospodarza. Aby wykonać operacje na niej z pomocą GPU musimy zaalokować pamięć na urządzeniu GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a następnie skopiować dane do GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuda.memcpy_htod(a_gpu, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy teraz na GPU zainicjalizowaną tablice, do ktorej możemy się z poziomu hosta (gospodarza) odwoływać przez `a_gpu`. Typową praktyką w programowaniu na GPU jest posiadania dwóch kopii danych - jednej w pamięci dostępnej dla CPU a drugą w pamięci urządzenia GPU. Warto jeszcze zaznaczyć, że nasza tablica `a_gpu` znajduje się w pamięci globlanej GPU.\n",
    "\n",
    "Teraz musimy napisać jądro, które będziemy wywoływać na urządzeniu na danych w pamięci globalnej GPU. Jądro jest napisane w zmodyfikowanej wersji języka C, zwanej CUDA-C. \n",
    "\n",
    "Jądro, które będzie mnożyło wszystkie elementy tablicy przez 2 ma następującą postać:\n",
    "\n",
    "```c\n",
    "     __global__ void dubluj(float *a)\n",
    "      {\n",
    "        int idx = threadIdx.x;\n",
    "        a[idx] *= 2;\n",
    "      }\n",
    "```\n",
    "     \n",
    "* Jądro wygląda jak zwykła funkcja w C.\n",
    "* Deklaracja `__global__` jest rozszerzeniem CUDA C.\n",
    "* Jądro będzie uruchomione w tylu kopiach ile jest elementów wektora `a_gpu` a każda będzie mnożyła jeden element wektora.\n",
    "* `threadIdx` jest strukturą określającą numer wątku wewnątrz bloku, który wykonuje daną kopię. Oprócz niego ważny jest też `blockIdx`, ale w tym przypadku odpalamy tylko jeden blok więc nie jest nam potrzebny. Indeksy wątku, czy też wewnątrz bloku czy numer bloku,  to jedyny sposób rozróżnienia wątków! Zmienna `idx` jest liniowym wskaźnikiem, który będzie się zmieniał od `0` do `N-1`.\n",
    "\n",
    "W pyCUDA źródło modułu podajemy jako `string` funkcji `SourceModule` w poniższy sposób:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void dubluj(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.y;\n",
    "    a[idx] *= 3.12;\n",
    "    \n",
    "  }\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmienna `mod` jest teraz obiektem, który ma m.in. funkcję `mod.get_function(\"nazwa\")` zwracającą funkcję pythonową, której wywołanie uruchomi odpowiednie jądro ze źródła uprzednio podanego do `SourceModule`. Sprawdźmy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func = mod.get_function(\"dubluj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołajmy teraz jądro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func(a_gpu, block=(16,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tablicy `a_gpu` powinniśmy mieć teraz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "[  3.11999989   6.23999977   9.35999966  12.47999954   5.           6.           7.\n",
      "   8.           9.          10.          11.          12.          13.          14.\n",
      "  15.          16.        ]\n",
      "[ 19.46879959  12.47999954  18.71999931  24.95999908  31.20000076\n",
      "  37.43999863  43.68000031  49.91999817  56.15999985  62.40000153\n",
      "  68.63999939  74.87999725  81.12000275  87.36000061  93.59999847\n",
      "  99.83999634]\n"
     ]
    }
   ],
   "source": [
    "print( a)\n",
    "cuda.memcpy_dtoh(a, a_gpu)\n",
    "print( a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że w tym programie równie dobrze można by wykonać następujące wywołanie:\n",
    "    \n",
    "    func(a_gpu, block=(4,4,1))\n",
    "    \n",
    "ale wtedy musimy zmodyfikować odpowiednio obliczanie wskaźnika `idx` w źródle jądra, na:\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "\n",
    "#### Ćwiczenie: \n",
    "\n",
    "Sprawdź to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udogodnienia w pyCUDA\n",
    "\n",
    "Powyższy sposób budowania programu w CUDA jest bardzo podobny gdybyśmy używali jedynie kompilatora oraz języka C/C++ zamiast pythona z pyCUDA. \n",
    "\n",
    "### InOut\n",
    "\n",
    "Warto zaznajomić się z kilkoma udogodnieniami, które mamy wbudowane w pyCUDA. Pierwszym z nich jest zautomatyzowanie procesu alokacji i kopiowania danych. Jeśli chcemy wykonać po kolei: alokacje wektora na GPU, transfer danych z wektora numpy, wywołanie jądra oraz nadpisanie wyjściowego wektora numpy wynikiem z GPU, to możemy użyć funkcji `InOut`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "[  3.11999989   6.23999977   9.35999966  12.47999954   5.           6.           7.\n",
      "   8.           9.          10.          11.          12.          13.          14.\n",
      "  15.          16.        ]\n"
     ]
    }
   ],
   "source": [
    "func(cuda.InOut(a), block=(4, 4, 1))\n",
    "print( a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gpuarray`\n",
    "\n",
    "Moduł `gpuarray` umożliwia bardzo zwarty zapis operacji na wektorach na GPU. Kluczową komendą jest `gpuarray.to_gpu(..`, która pozwala skopiować wektor numpy do urządzenia GPU, zwracając uchwyt do danych na GPU. Pewne operacje można wykonać automatycznie na GPU po prostu wpisując wzór arytmetyczny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.\n",
      "  16.]\n",
      "[  2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.  26.  28.  30.\n",
      "  32.]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "\n",
    "a = np.linspace(1,16,16).astype(np.float32)\n",
    "a_gpu = gpuarray.to_gpu(a.astype(np.float32))\n",
    "a_doubled = (a_gpu*2).get()\n",
    "\n",
    "print (a_gpu)\n",
    "print (a_doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uwaga printf - działa tylko z konsoli!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cuda_printf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cuda_printf.py\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "    #include <stdio.h>\n",
    "\n",
    "    __global__ void printf_test()\n",
    "    {\n",
    "      printf(\"GONZO %d.%d.%d\\\\n\", threadIdx.x, threadIdx.y, threadIdx.z);\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "func = mod.get_function(\"printf_test\")\n",
    "func(block=(2,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grids & Blocks \n",
    "W CUDA wątki można uruchamiać w blokach (blocks). Należy zapamiętać regułe:\n",
    "\n",
    "**wszystkie wątki na jednym bloku zawsze będą uruchamiane na tym samym multiprocesorze**\n",
    "           \n",
    "Co za tym idzie, będą miały do dyspozycji tą samą pamięc typu \"shared memory\". Maksymalna liczba wątków w jednym bloku jest z reguły ograniczona do 512 lub 1024 w zależności od typu urządzenia GPU. Warto zauważyć, że jest to liczba zdecydowanie większa od liczby procesorów skalarnych w na jednym multiprocesorze. \n",
    "\n",
    "![Organizacja numeracji wątków](files/figs/grid_blocks.jpg) \n",
    "\n",
    "Jeżeli chcemy odpalic np. milion wątków, to musimy wykorzystać tzw. grid bloków. Czyli odpalając jądro obliczeniowe specyfikujemy ile potrzeujemy wątków i ile jaki chcemy mieć rozmiar bloku. Chcąc mieć np. $128\\times 64$ wątki, rozsądnym jest odpalenie jądra z rozmiarem gridu $128$ i rozmiarem bloku $64$.   \n",
    "\n",
    "Ponadto, ponieważ często operujemy siatkami reprezentujemy dwu i trój-wymiarowe pola, CUDA ma wbudowany mechanizm, który ułatwia posługiwaniem się dwu i trój-wymiarowymi siatkami. \n",
    "\n",
    "Poniższy przykład pokazuje jak zbudować grid składający się z dwóch bloków, po pięc wątków.\n",
    "\n",
    "#### Ćwiczenie.       \n",
    "Zastąp w kodzie  `if(threadIdx.x==1)` przez:\n",
    "\n",
    "* `if(blockIdx.x==1)`\n",
    "* `if(idx==1)`\n",
    "\n",
    "i sprawdź działanie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      "----------------\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[  1.  10.  20.  30.  40.  51.  60.  70.  80.  90.]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void kernel(float *a)\n",
    "    {\n",
    "      int idx = threadIdx.x + blockDim.x*blockIdx.x;\n",
    "    \n",
    "      if(threadIdx.x==0)\n",
    "          a[idx] += 1.0f;\n",
    "      a[idx] += 10.0f * idx;\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "a = np.zeros(10).astype(np.float32)\n",
    "func = mod.get_function(\"kernel\")\n",
    "print (np.linspace(0,9,10))\n",
    "print (\"----------------\")\n",
    "print (a)\n",
    "func(cuda.InOut(a),block=(5,1,1),grid=(2,1,1))\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co dalej?\n",
    "\n",
    "Analizując kod powyższych przykładów, jesteśmy w stanie samodzielnie napisać elementarne jądro, je wywołać i odczytać dane wynikowe. \n",
    "\n",
    "Proponujemy teraz przejść do przykładów, które umożliwią zapoznanie się z ogromnymi możliwościamy procesora GPU w zastosowaniach typowych dla nauk ścisłych.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
